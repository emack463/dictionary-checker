import os
import sqlite3
from typing import List, Dict, Set, Optional, Tuple
import json


class DictionaryChecker:
    """Check tokens against an English dictionary in SQLite database."""
    
    def __init__(self, db_path: str, use_stemming: bool = False, stemmer=None, handle_hyphenated: bool = True):
        """Initialize the dictionary checker.
        
        Args:
            db_path: Path to the SQLite database file
            use_stemming: Whether to check stem forms if original not found
            stemmer: Optional TokenStemmer instance for stem checking
            handle_hyphenated: Whether to use special handling for hyphenated words
        """
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.cursor = self.conn.cursor()
        self.use_stemming = use_stemming
        self.stemmer = stemmer
        self.handle_hyphenated = handle_hyphenated
        
        # Cache for performance
        self._word_cache: Set[str] = set()
        self._cache_initialized = False
        
        # Statistics for stem-based matches
        self.stem_match_stats = {
            'original_matches': 0,
            'stem_matches': 0,
            'hyphenated_matches': 0,
            'no_matches': 0
        }
        
        # Hyphenated word handler (lazy initialization)
        self._hyphenated_handler = None
    
    def _initialize_cache(self):
        """Load all dictionary words into memory for faster lookups."""
        if self._cache_initialized:
            return
        
        print("Loading dictionary into memory...")
        self.cursor.execute("SELECT DISTINCT LOWER(word) FROM entries")
        self._word_cache = {row[0] for row in self.cursor.fetchall()}
        self._cache_initialized = True
        print(f"Loaded {len(self._word_cache):,} unique words")
    
    def is_in_dictionary(self, word: str, case_sensitive: bool = False, check_stem: bool = None) -> Tuple[bool, str]:
        """Check if a word exists in the dictionary.
        
        Args:'hyphenated', or 'none'
        """
        self._initialize_cache()
        
        check_word = word if case_sensitive else word.lower()
        
        # Check original form first
        if check_word in self._word_cache:
            self.stem_match_stats['original_matches'] += 1
            return (True, 'original')
        
        # Check hyphenated word handling
        if self.handle_hyphenated and '-' in word:
            if self._hyphenated_handler is None:
                # Lazy initialization
                import sys
                import os
                here = os.path.dirname(os.path.abspath(__file__))
                if here not in sys.path:
                    sys.path.insert(0, here)
                from hyphenated_handler import HyphenatedWordHandler
                self._hyphenated_handler = HyphenatedWordHandler(self, self.stemmer)
            
            is_valid, match_type, details = self._hyphenated_handler.is_valid_hyphenated_word(word)
            if is_valid:
                self.stem_match_stats['hyphenated_matches'] += 1
                return (True, f'hyphenated_{match_type}')
        
        check_word = word if case_sensitive else word.lower()
        
        # Check original form first
        if check_word in self._word_cache:
            self.stem_match_stats['original_matches'] += 1
            return (True, 'original')
        
        # Check stem form if enabled
        if check_stem is None:
            check_stem = self.use_stemming
        
        if check_stem and self.stemmer:
            stem = self.stemmer.stem_token(word)
            if stem != check_word and stem in self._word_cache:
                self.stem_match_stats['stem_matches'] += 1
                return (True, 'stem')
        
        self.stem_match_stats['no_matches'] += 1
        return (False, 'none')
    
    def check_tokens(self, tokens: List[str]) -> Dict[str, bool]:
        """Check a list of tokens against the dictionary.
        
        Args:
            tokens: List of tokens to check
            
        Returns:
            Dictionary mapping each unique token to True/False (in dictionary or not)
        """
        self._initialize_cache()
        
        results = {}
        for token in tokens:
            # Check lowercase version
            token_lower = token.lower()
            if token_lower not in results:
                found, _ = self.is_in_dictionary(token_lower)
                results[token_lower] = found
        hyphenated_found_tokens = []  # Found via hyphenated word handling
        
        # Track match types
        match_details = []
        
        for token in tokens:
            token_lower = token.lower()
            
            # Check original form first
            if token_lower in self._word_cache:
                found_tokens.append(token)
                match_details.append({'token': token, 'match_type': 'original', 'stem': None})
            else:
                # Check hyphenated word handling
                hyphenated_matched = False
                if self.handle_hyphenated and '-' in token:
                    if self._hyphenated_handler is None:
                        # Lazy initialization
                        import sys
                        import os
                        here = os.path.dirname(os.path.abspath(__file__))
                        if here not in sys.path:
                            sys.path.insert(0, here)
                        from hyphenated_handler import HyphenatedWordHandler
                        self._hyphenated_handler = HyphenatedWordHandler(self, self.stemmer)
                    
                    is_valid, match_type, details = self._hyphenated_handler.is_valid_hyphenated_word(token)
                    if is_valid:
                        hyphenated_found_tokens.append(token)
                        match_details.append({'token': token, 'match_type': f'hyphenated_{match_type}', 'details': details})
                        hyphenated_matched = True
                
                # Check stem form if not found via hyphenation
                if not hyphenated_matched:
                    stem_matched = False
                    stem_form = None
                    
                    if self.use_stemming and self.stemmer:
                        stem_form = self.stemmer.stem_token(token)
                        if stem_form != token_lower and stem_form in self._word_cache:
                            stem_found_tokens.append(token)
                            match_details.append({'token': token, 'match_type': 'stem', 'stem': stem_form})
                            stem_matched = True
                    
                    if not stem_matched:
                        not_found_tokens.append(token)
                if token_lower in self._word_cache:
               hyphenated_found = set(t.lower() for t in hyphenated_found_tokens)
        unique_not_found = set(t.lower() for t in not_found_tokens)
        
        # Combined found (original + stem + hyphenated)
        all_found = found_tokens + stem_found_tokens + hyphenated_found_tokens
        unique_all_found = unique_found | unique_stem_found | unique_hyphenated_found
        
        result = {
            'total_tokens': len(tokens),
            'found_count': len(found_tokens),
            'stem_found_count': len(stem_found_tokens),
            'hyphenated_found_count': len(hyphenated_found_tokens),
            'combined_found_count': len(all_found),
            'not_found_count': len(not_found_tokens),
            'unique_found': len(unique_found),
            'unique_stem_found': len(unique_stem_found),
            'unique_hyphenated_found': len(unique_hyphenated_found),
            'unique_combined_found': len(unique_all_found),
            'unique_not_found': len(unique_not_found),
            'found_percentage': (len(found_tokens) / len(tokens) * 100) if tokens else 0,
            'combined_found_percentage': (len(all_found) / len(tokens) * 100) if tokens else 0,
            'stem_contribution': (len(stem_found_tokens) / len(tokens) * 100) if tokens else 0,
            'hyphenated_contribution': (len(hyphenated_found_tokens) / len(tokens) * 100) if tokens else 0,
            'found_tokens': found_tokens,
            'stem_found_tokens': stem_found_tokens,
            'hyphenated_found_tokens': hyphenated_found_tokens,
            'not_found_tokens': not_found_tokens,
            'unique_found_list': sorted(unique_found),
            'unique_stem_found_list': sorted(unique_stem_found),
            'unique_hyphenated_found_list': sorted(unique_hyphenated
        all_found = found_tokens + stem_found_tokens
        unique_all_found = unique_found | unique_stem_found
        
        result = {
            'total_tokens': len(tokens),
            'found_count': len(found_tokens),
            'stem_found_count': len(stem_found_tokens),
            'combined_found_count': len(all_found),
            'not_found_count': len(not_found_tokens),
            'unique_found': len(unique_found),
            'unique_stem_found': len(unique_stem_found),
            'unique_combined_found': len(unique_all_found),
            'unique_not_found': len(unique_not_found),
            'found_percentage': (len(found_tokens) / len(tokens) * 100) if tokens else 0,
            'combined_found_percentage': (len(all_found) / len(tokens) * 100) if tokens else 0,
            'stem_contribution': (len(stem_found_tokens) / len(tokens) * 100) if tokens else 0,
            'found_tokens': found_tokens,
            'stem_found_tokens': stem_found_tokens,
            'not_found_tokens': not_found_tokens,
            'unique_found_list': sorted(unique_found),
            'unique_stem_found_list': sorted(unique_stem_found),
            'unique_not_found_list': sorted(unique_not_found),
            'match_details': match_details
        }
        
        # Add stemming stats if enabled
        if self.use_stemming:
            result['stemming_stats'] = self.stem_match_stats.copy()
        
        return result
    
    def close(self):
        """Close the database connection."""
        if self.conn:
            self.conn.close()
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()


def check_tokenized_files(tokenized_json_path: str, db_path: str, output_path: Optional[str] = None, 
                         use_stemming: bool = False, stem_method: str = 'snowball') -> Dict:
    """Check all tokenized files against the dictionary.
    
    Args:
        tokenized_json_path: Path to the tokenized summary JSON file
        db_path: Path to the SQLite dictionary database
        output_path: Optional path to save results JSON
        use_stemming: Whether to use stemming for dictionary lookups (default: False)
        stem_method: Stemming method - 'snowball' (default), 'porter', or 'lemmatize'
        
    Returns:
        Dictionary with complete analysis results
    """
    # Load tokenized data
    print(f"Loading tokenized data from: {tokenized_json_path}")
    with open(tokenized_json_path, 'r', encoding='utf-8') as f:
        tokenized_data = json.load(f)
    
    print(f"Files to process: {tokenized_data['total_files']}")
    print(f"Total tokens: {tokenized_data['total_tokens']:,}")
    
    # Initialize stemmer if needed
    stemmer = None
    if use_stemming:
        print(f"Initializing {stem_method} stemmer...")
        # Import from the same directory
        import sys
        import os
        here = os.path.dirname(os.path.abspath(__file__))
        if here not in sys.path:
            sys.path.insert(0, here)
        from stemmer import TokenStemmer
        stemmer = TokenStemmer(method=stem_method)
        print("+ Stemmer ready\n")
    else:
        print()
    
    # Initialize dictionary checker
    with DictionaryChecker(db_path, use_stemming=use_stemming, stemmer=stemmer) as checker:
        # Initialize cache first to get correct count
        checker._initialize_cache()
        
        results = {
            'summary': {
                'total_files': tokenized_data['total_files'],
                'total_tokens': tokenized_data['total_tokens'],
                'dictionary_words': len(checker._word_cache),
                'stemming_enabled': use_stemming,
                'stem_method': stem_method if use_stemming else None
            },
            'files': {}
        }
        
        # Process each file
        file_count = 0
        for filename, file_data in tokenized_data['files'].items():
            file_count += 1
            tokens = file_data['tokens']
            
            # Analyze tokens
            analysis = checker.analyze_tokens(tokens)
            
            # Store results (without full token lists to save space)
            file_results = {
                'total_tokens': analysis['total_tokens'],
                'found_count': analysis['found_count'],
                'not_found_count': analysis['not_found_count'],
                'unique_found': analysis['unique_found'],
                'unique_not_found': analysis['unique_not_found'],
                'found_percentage': analysis['found_percentage'],
                'not_found_tokens': analysis['unique_not_found_list']  # Keep list of unknown words
            }
            
            # Add stemming-specific results if enabled
            if use_stemming or checker.handle_hyphenated:
                if 'stem_found_count' in analysis:
                    file_results['stem_found_count'] = analysis['stem_found_count']
                if 'hyphenated_found_count' in analysis:
                    file_results['hyphenated_found_count'] = analysis['hyphenated_found_count']
                if 'combined_found_count' in analysis:
                    file_results['combined_found_count'] = analysis['combined_found_count']
                    file_results['combined_found_percentage'] = analysis['combined_found_percentage']
                if 'stem_contribution' in analysis:
                    file_results['stem_contribution'] = analysis['stem_contribution']
                if 'hyphenated_contribution' in analysis:
                    file_results['hyphenated_contribution'] = analysis['hyphenated_contribution']
                if 'unique_stem_found' in analysis:
                    file_results['unique_stem_found'] = analysis['unique_stem_found']
                    file_results['stem_found_tokens'] = analysis['unique_stem_found_list']
                if 'unique_hyphenated_found' in analysis:
                    file_results['unique_hyphenated_found'] = analysis['unique_hyphenated_found']
                    file_results['hyphenated_found_tokens'] = analysis['unique_hyphenated_found_list']
            
            results['files'][filename] = file_results
            
            if file_count % 100 == 0:
                print(f"Processed {file_count}/{tokenized_data['total_files']} files...")
        
        # Calculate overall statistics
        total_found = sum(f['found_count'] for f in results['files'].values())
        total_not_found = sum(f['not_found_count'] for f in results['files'].values())
        
        # Collect all unique not-found words across all files
        all_not_found = set()
        for file_data in results['files'].values():
            all_not_found.update(file_data['not_found_tokens'])
        
        results['summary']['total_found'] = total_found
        results['summary']['total_not_found'] = total_not_found
        results['summary']['found_percentage'] = (total_found / (total_found + total_not_found) * 100) if (total_found + total_not_found) > 0 else 0
        results['summary']['unique_not_found_words'] = len(all_not_found)
        results['summary']['all_not_found_words'] = sorted(all_not_found)
        
        # Add stemming and hyphenated summary stats
        if use_stemming or checker.handle_hyphenated:
            total_stem_found = sum(f.get('stem_found_count', 0) for f in results['files'].values())
            total_hyphenated_found = sum(f.get('hyphenated_found_count', 0) for f in results['files'].values())
            total_combined_found = sum(f.get('combined_found_count', 0) for f in results['files'].values())
            
            all_stem_found = set()
            all_hyphenated_found = set()
            for file_data in results['files'].values():
                if 'stem_found_tokens' in file_data:
                    all_stem_found.update(file_data['stem_found_tokens'])
                if 'hyphenated_found_tokens' in file_data:
                    all_hyphenated_found.update(file_data['hyphenated_found_tokens'])
            
            if total_stem_found > 0:
                results['summary']['total_stem_found'] = total_stem_found
                results['summary']['stem_contribution'] = (total_stem_found / tokenized_data['total_tokens'] * 100)
                results['summary']['unique_stem_found_words'] = len(all_stem_found)
            
            if total_hyphenated_found > 0:
                results['summary']['total_hyphenated_found'] = total_hyphenated_found
                results['summary']['hyphenated_contribution'] = (total_hyphenated_found / tokenized_data['total_tokens'] * 100)
                results['summary']['unique_hyphenated_found_words'] = len(all_hyphenated_found)
            
            if total_combined_found > 0:
                results['summary']['total_combined_found'] = total_combined_found
                results['summary']['combined_found_percentage'] = (total_combined_found / (total_combined_found + total_not_found) * 100) if (total_combined_found + total_not_found) > 0 else 0
            
            results['summary']['stemming_stats'] = checker.stem_match_stats.copy()
    
    # Save results if output path provided
    if output_path:
        print(f"\nSaving results to: {output_path}")
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2)
        print(f"File size: {os.path.getsize(output_path):,} bytes")
    
    return results


if __name__ == "__main__":
    here = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(here, '..'))
    
    db_path = os.path.join(project_root, 'data', 'dictionary.db')
    tokenized_path = os.path.join(project_root, 'data', 'tokenized_summary.json')
    output_path = os.path.join(project_root, 'data', 'dictionary_check_results.json')
    
    print("="*60)
    print("Dictionary Check")
    print("="*60)
    print()
    
    results = check_tokenized_files(tokenized_path, db_path, output_path)
    
    print("\n" + "="*60)
    print("Summary")
    print("="*60)
    print(f"Total tokens checked: {results['summary']['total_tokens']:,}")
    print(f"Tokens found in dictionary: {results['summary']['total_found']:,} ({results['summary']['found_percentage']:.2f}%)")
    print(f"Tokens NOT in dictionary: {results['summary']['total_not_found']:,}")
    print(f"Unique unknown words: {results['summary']['unique_not_found_words']:,}")
    print()
    print("Sample unknown words:")
    for word in results['summary']['all_not_found_words'][:20]:
        print(f"  - {word}")
